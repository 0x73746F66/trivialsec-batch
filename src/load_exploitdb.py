import atexit
import logging
import json
import argparse
import pathlib
from random import randint
from time import sleep
from datetime import datetime, timedelta
import requests
from requests.exceptions import ConnectTimeout, ReadTimeout, ConnectionError
from botocore.errorfactory import ClientError
from retry.api import retry
from elasticsearch import Elasticsearch
from trivialsec.models.cve import CVE
from trivialsec.helpers.config import config


session = requests.Session()
logger = logging.getLogger(__name__)
logging.basicConfig(
    format='%(asctime)s - %(name)s - [%(levelname)s] %(message)s',
    level=logging.INFO
)
BASE_URL = 'https://www.exploit-db.com/'
DATAFILE_DIR = '/var/cache/trivialsec/exploitdb-submission'
RAWFILE_DIR = '/var/cache/trivialsec/exploitdb-raw'
DATE_FMT = '%Y-%m-%d'
PROXIES = None
if config.http_proxy or config.https_proxy:
    PROXIES = {
        'http': f'http://{config.http_proxy}',
        'https': f'https://{config.https_proxy}'
    }
DEFAULT_START_YEAR = 2011
DEFAULT_INDEX = 'cves'
REPORT = {
    'task': 'exploitdb-submissions',
    'total': 0,
    'skipped': 0,
    'updates': 0,
    'new': 0,
}

def save_submission(edb_data :dict):
    for cve_ref in edb_data.get('cve', []):
        REPORT['total'] += 1
        update = False
        save = False
        cve = CVE(cve_id=cve_ref)
        original_cve = CVE(cve_id = cve.cve_id)
        if cve.hydrate():
            update = True
            original_cve.hydrate()
        else:
            logger.warning(f'Official {cve_ref} missing from our Database')
            cve.assigner = 'Unknown'
            cve.description = f"Exploit submitted by {edb_data['author']}"
            cve.cvss_version = '3.1'
            cve.vector = 'CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:N'
            save = True

        exploits = set()
        cve.exploit = []
        for exploit in original_cve.exploit:
            if exploit.get('source_id') not in exploits:
                exploits.add(exploit.get('source_id'))
                cve.exploit.append(exploit)
        if edb_data['edb_id'] not in exploits:
            exploits.add(edb_data['edb_id'])
            cve.exploit.append({
                'cve_id': cve_ref,
                'source': 'exploit-db',
                'source_id': edb_data['edb_id'],
                'source_url': edb_data['url'],
                'title': edb_data['title'],
                'created_at': edb_data['created_at'],
                'published_at': edb_data['published'],
                'author': edb_data['author'],
                'author_url': edb_data['author_url'],
                'platform': edb_data.get('platform'),
                'type': edb_data.get('type'),
                'verified': edb_data.get('verified', False),
                'exploit_exists': edb_data.get('exploit_exists', False),
                'download_url': edb_data.get('download_url'),
            })
            save = True

        reference_urls = set()
        cve.references = []
        for reference in original_cve.references:
            if reference.get('url') not in reference_urls:
                reference_urls.add(reference.get('url'))
                cve.references.append(reference)

        if edb_data['url'] not in reference_urls:
            reference_urls.add(edb_data['url'])
            cve.references.append({
                "url": edb_data['url'],
                "name": f"Exploit submission by {edb_data['author']}",
                "source": "exploit-db",
                "tags": ["Exploit"]
            })
            save = True

        if cve.title is None:
            cve.title = edb_data['title']
            save = True
        if cve.reported_at is None:
            cve.reported_at = edb_data['created_at']
            save = True
        if cve.published_at is None:
            cve.published_at = edb_data['published']
            save = True

        if cve.cvss_version in ['2.0']:
            vec = CVE.vector_to_dict(cve.vector, 2)
            vec['E'] = 'POC'
            vec['RC'] = 'UC'
            if edb_data['exploit_exists'] is True:
                vec['E'] = 'F'
                vec['RC'] = 'UR'
                save = True
            if edb_data.get('verified', False):
                vec['E'] = 'H'
                vec['RC'] = 'C'
                save = True
            cve.vector = CVE.dict_to_vector(vec, 2)
        if cve.cvss_version in ['3.0', '3.1']:
            vec = CVE.vector_to_dict(cve.vector, 3)
            vec['E'] = 'P'
            vec['RC'] = 'U'
            if edb_data['exploit_exists'] is True:
                vec['E'] = 'F'
                vec['RC'] = 'R'
                save = True
            if edb_data.get('verified', False):
                vec['E'] = 'H'
                vec['RC'] = 'C'
                save = True
            cve.vector = CVE.dict_to_vector(vec, 3)

        if save is True:
            REPORT['new' if update is False else 'updates'] += 1
            extra = None
            doc = cve.get_doc()
            if doc is not None:
                extra = {
                    '_nvd': doc.get('_source', {}).get('_nvd'),
                    '_xforce': doc.get('_source', {}).get('_xforce'),
                    '_exploitdb': edb_data.get('_raw'),
                }
            cve.persist(extra=extra)

@retry((ConnectTimeout, ReadTimeout, ConnectionError), tries=10, delay=30, backoff=5)
def query_raw(ref_id :int):
    api_url = f'{BASE_URL}raw/{ref_id}'
    logger.info(api_url)
    resp = requests.get(
        api_url,
        proxies=PROXIES,
        headers={
            'user-agent': config.user_agent,
            'referer': f'{BASE_URL}exploits/{ref_id}'
        },
        timeout=3
    )
    if resp.status_code != 200:
        logger.error(f'status_code {resp.status_code}')
        return None

    return resp.text

@retry((ConnectTimeout, ReadTimeout, ConnectionError), tries=10, delay=30, backoff=5)
def query_bulk(start :int, length :int = 15):
    timestamp = int((datetime.utcnow() - datetime(1970, 1, 1)) / timedelta(seconds=1)*1000)
    edb_url = f'{BASE_URL}?draw=2&columns%5B0%5D%5Bdata%5D=date_published&columns%5B0%5D%5Bname%5D=date_published&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=download&columns%5B1%5D%5Bname%5D=download&columns%5B1%5D%5Bsearchable%5D=false&columns%5B1%5D%5Borderable%5D=false&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=application_md5&columns%5B2%5D%5Bname%5D=application_md5&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=false&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=verified&columns%5B3%5D%5Bname%5D=verified&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=false&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=description&columns%5B4%5D%5Bname%5D=description&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=false&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=type_id&columns%5B5%5D%5Bname%5D=type_id&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=false&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=platform_id&columns%5B6%5D%5Bname%5D=platform_id&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=false&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=author_id&columns%5B7%5D%5Bname%5D=author_id&columns%5B7%5D%5Bsearchable%5D=false&columns%5B7%5D%5Borderable%5D=false&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=code&columns%5B8%5D%5Bname%5D=code.code&columns%5B8%5D%5Bsearchable%5D=true&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B9%5D%5Bdata%5D=id&columns%5B9%5D%5Bname%5D=id&columns%5B9%5D%5Bsearchable%5D=false&columns%5B9%5D%5Borderable%5D=true&columns%5B9%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B9%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=9&order%5B0%5D%5Bdir%5D=desc&start={start}&length={length}&search%5Bvalue%5D=&search%5Bregex%5D=false&author=&port=&type=&tag=&platform=&_={timestamp}'
    logger.info(f'start={start} length={length} timestamp={timestamp}')
    try:
        resp = session.get(
            edb_url,
            proxies=PROXIES,
            headers={
                'x-requested-with': 'XMLHttpRequest',
                'user-agent': config.user_agent,
                'referer': BASE_URL
            },
            allow_redirects=False,
            timeout=10
        )
        code = resp.status_code
    except ReadTimeout:
        code = 504

    if code != 200:
        logger.warning(f'{code} {edb_url}')
        return None

    return json.loads(resp.text)

def normalise_edb(data :dict):
    del data['download']
    cve = []
    if data['code']:
        for code in data['code']:
            # find $DATAFILE_DIR -name '*.json' -exec jq -r '._raw.code[].code_type' {} \; 2>/dev/null
            if code['code_type'].lower() == 'cve':
                cve.append(f"CVE-{code['code'].upper().replace('CVE‑', '').replace('VE‑', '').replace('CVE ', '').replace('‑', '-').replace('–', '-')}")

    submission_data = {
        "_raw": data,
        "edb_id": int(data['id']),
        "verified": data['verified'] == 1,
        "url": f"{BASE_URL}exploits/{data['id']}",
        "cve": cve,
        "published": data['date_published'],
        "title": data['description'][1],
        "author": data['author']['name'],
        "author_url": f"{BASE_URL}?author={data['author']['id']}",
        "platform": data['platform']['platform'],
        "type": data['type']['display'],
        "download_url": f"{BASE_URL}download/{data['id']}",
        "created_at": datetime.utcnow().replace(microsecond=0).isoformat(),
    }
    s3_client = config.boto3_session.client(service_name='s3')
    def s3_file_exist(**kwargs) -> bool:
        try:
            s3_client.head_object(**kwargs)
            return True
        except ClientError as ex:
            if ex.response['Error']['Code'] == "404":
                return False
            else:
                raise
    s3_path = f"{config.aws['env_prefix']}/intel/exploitdb/submission-{data['id']}"
    submission_data['exploit_exists'] = s3_file_exist(Bucket=config.aws['public_bucket'], Key=s3_path)
    if submission_data['exploit_exists'] is False:
        sleep(randint(3,6))
        raw = query_raw(data['id'])
        if raw is not None and raw:
            s3_client.put_object(Bucket=config.aws['public_bucket'], Key=s3_path, Body=raw)
            submission_data['exploit_exists'] = True

    return submission_data

def do_bulk(not_before :datetime, force :bool = False):
    limit = 1000
    response = query_bulk(0, limit)
    if response is None:
        REPORT['total'] += 1
        REPORT['skipped'] += 1
        return
    data = response.get('data', [])
    if not data:
        REPORT['total'] += 1
        REPORT['skipped'] += 1
        return
    for edb in data:
        edb_data = normalise_edb(edb)
        save_submission(edb_data)

    break_on = int(data[0]['id']) + limit
    start = limit
    while start <= break_on:
        sleep(randint(4,8))
        response = query_bulk(start, limit)
        if response is None:
            REPORT['total'] += 1
            REPORT['skipped'] += 1
            continue
        data = response.get('data', [])
        if not data:
            REPORT['total'] += 1
            REPORT['skipped'] += 1
            start += limit
            continue
        for edb in data:
            published = datetime.strptime(edb['date_published'], DATE_FMT)
            if force is False and published < not_before:
                logger.info(f'{published} < {not_before}')
                REPORT['total'] += 1
                REPORT['skipped'] += 1
                continue
            edb_data = normalise_edb(edb)
            save_submission(edb_data)
        start += limit

def do_latest(not_before :datetime, force :bool = False, limit :int = 1000):
    response = query_bulk(0, limit)
    if response is None:
        logger.info('response is None')
        REPORT['total'] += 1
        REPORT['skipped'] += 1
        return
    data = response.get('data', [])
    if not data:
        logger.info('data is None')
        REPORT['total'] += 1
        REPORT['skipped'] += 1
        return
    for edb in data:
        published = datetime.strptime(edb['date_published'], DATE_FMT)
        if force is False and published < not_before:
            logger.info(f'{published} < {not_before}')
            REPORT['total'] += 1
            REPORT['skipped'] += 1
            continue
        edb_data = normalise_edb(edb)
        save_submission(edb_data)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('-i', '--index', help='Elasticsearch index', dest='index', default=DEFAULT_INDEX)
    parser.add_argument('-y', '--since-year', help='optionally specify a year to start from', dest='year', default=DEFAULT_START_YEAR)
    parser.add_argument('--not-before', help='ISO format datetime string to skip all RSS records published until this time', dest='not_before', default=None)
    parser.add_argument('-r', '--recent', help='Process the latest 1-1000 max published records (default 1000) change limit using "--recent-limit"', dest='process_latest', action="store_true")
    parser.add_argument('-l', '--recent-limit', help='Used with "--recent" set between 1-1000 max (default 1000)', dest='latest_limit', default=1000)
    parser.add_argument('-f', '--force-process', help='Force processing all records', dest='force', action="store_true")
    parser.add_argument('-s', '--only-show-errors', help='set logging level to ERROR (default CRITICAL)', dest='log_level_error', action="store_true")
    parser.add_argument('-q', '--quiet', help='set logging level to WARNING (default CRITICAL)', dest='log_level_warning', action="store_true")
    parser.add_argument('-v', '--verbose', help='set logging level to INFO (default CRITICAL)', dest='log_level_info', action="store_true")
    parser.add_argument('-vv', '--debug', help='set logging level to DEBUG (default CRITICAL)', dest='log_level_debug', action="store_true")
    args = parser.parse_args()
    log_level = logging.CRITICAL
    if args.log_level_error:
        log_level = logging.ERROR
    if args.log_level_warning:
        log_level = logging.WARNING
    if args.log_level_info:
        log_level = logging.INFO
    if args.log_level_debug:
        log_level = logging.DEBUG
    logging.basicConfig(
        format='%(asctime)s - %(name)s - [%(levelname)s] %(message)s',
        level=log_level
    )
    es = Elasticsearch(
        config.elasticsearch.get('hosts'),
        http_auth=(config.elasticsearch.get('user'), config.elasticsearch_password),
        scheme=config.elasticsearch.get('scheme'),
        port=config.elasticsearch.get('port'),
    )
    es.indices.create(index=args.index, ignore=400)
    not_before = datetime(year=int(args.year), month=1 , day=1)
    if args.not_before is not None:
        not_before = datetime.strptime(args.not_before, DATE_FMT)
    if args.process_latest is True:
        do_latest(not_before, args.force, limit=int(args.latest_limit))
    else:
        do_bulk(not_before, args.force)

def report():
    end = datetime.utcnow()
    epoch = datetime(1970,1,1)
    elapsed = (end-epoch).total_seconds()-(start-epoch).total_seconds()
    REPORT['start'] = str(start)
    REPORT['end'] = str(end)
    REPORT['elapsed'] = str(timedelta(seconds=elapsed))
    print(repr(REPORT))

if __name__ == "__main__":
    start = datetime.utcnow()
    atexit.register(report)
    main()
